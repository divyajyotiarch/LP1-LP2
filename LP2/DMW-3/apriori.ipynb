{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple dataset of transactions\n",
    "def createDataSet():\n",
    "    return [[1, 3, 4], [2, 3, 5], [1, 2, 3, 5], [2, 5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createC1(dataSet):\n",
    "    C1 = []\n",
    "    for transaction in dataSet:\n",
    "        for item in transaction:\n",
    "            if not [item] in C1:\n",
    "                C1.append([item])\n",
    "                \n",
    "    C1.sort()\n",
    "    return list(map(frozenset, C1)) # use frozen set so we\n",
    "                            #can use it as a key in a dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ck - List of candidates\n",
    "# minSupport - Minimum support value that the candidate sets need to meet\n",
    "# This function generates the first set of candidates (L1) that meet the required support values\n",
    "# It also returns a dictionary with support values of the L1 set\n",
    "def scanDataSet(dataset, Ck, minSupport):\n",
    "    ssCnt = {}\n",
    "    for tid in dataset:\n",
    "        for can in Ck:\n",
    "            if can.issubset(tid):\n",
    "                if not can in ssCnt: \n",
    "                    ssCnt[can] = 1\n",
    "                else: \n",
    "                    ssCnt[can] += 1\n",
    "    numItems = float(len(dataset))\n",
    "    retList = []\n",
    "    supportData = {}\n",
    "    for key in ssCnt:\n",
    "        support = ssCnt[key] / numItems\n",
    "        if support >= minSupport:\n",
    "            retList.insert(0,key)\n",
    "        # add the support for items that did not make the cut for debugging purposes\n",
    "        supportData[key] = support\n",
    "    return retList, supportData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = createDataSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 3, 4], [2, 3, 5], [1, 2, 3, 5], [2, 5]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the list of transactions into a list of single item sets\n",
    "# It returns a frozenset to ensure that the entries in the set can be used as keys in a dictionary\n",
    "# since frozenset values are immutable\n",
    "C1 = createC1(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[frozenset({1}),\n",
       " frozenset({2}),\n",
       " frozenset({3}),\n",
       " frozenset({4}),\n",
       " frozenset({5})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the transactions into a set and returns a list of sets\n",
    "# This is needed because it is easier to work with sets in the apriori algorithm\n",
    "# rather than with a list of items in a transaction\n",
    "# This is the dataset we will be working with from now\n",
    "D = list(map(set, dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{1, 3, 4}, {2, 3, 5}, {1, 2, 3, 5}, {2, 5}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have set minimum support value as 0.5\n",
    "L1, supportData0 = scanDataSet(D, C1, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[frozenset({5}), frozenset({2}), frozenset({3}), frozenset({1})]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{frozenset({1}): 0.5,\n",
       " frozenset({3}): 0.75,\n",
       " frozenset({4}): 0.25,\n",
       " frozenset({2}): 0.75,\n",
       " frozenset({5}): 0.75}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can see that since 4 has a support of less than 0.5, it has not been included in the L1 candidate set\n",
    "supportData0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes a list of frequent itemsets (Lk) and the size of itemsets (k)\n",
    "# It produces the new candidate set Ck\n",
    "# For example, it will take itemsets {0}, {1}, {2} and produce {0, 1}, {0, 2} and {1, 2}\n",
    "def CandidateSetGenerator(Lk, k):\n",
    "    retList = []\n",
    "    lenLk = len(Lk)\n",
    "    for i in range(lenLk): # Consider one item and a time\n",
    "        for j in range(i+1, lenLk): # Considers all items after \"ith\" item in the list\n",
    "            L1 = list(Lk[i])[:k-2]\n",
    "            L2 = list(Lk[j])[:k-2]\n",
    "            L1.sort()\n",
    "            L2.sort()\n",
    "            if L1==L2: #if first k-2 elements are equal\n",
    "                retList.append(Lk[i] | Lk[j]) # This is the set union operator in Python\n",
    "    return retList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aprioriAlgorithm(dataset, minSupport = 0.5):\n",
    "    C1 = createC1(dataset)\n",
    "    D = list(map(set, dataset))\n",
    "    L1, supportData = scanDataSet(D, C1, minSupport)\n",
    "    L = [L1]\n",
    "    k = 2\n",
    "    while (len(L[k-2]) > 0):\n",
    "        Ck = CandidateSetGenerator(L[k-2], k)\n",
    "        Lk, supK = scanDataSet(D, Ck, minSupport) # scan transactions to get Lk\n",
    "        supportData.update(supK) # Replace the support data with \n",
    "        L.append(Lk)\n",
    "        k += 1\n",
    "    return L, supportData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "L,suppData = aprioriAlgorithm(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[frozenset({5}), frozenset({2}), frozenset({3}), frozenset({1})],\n",
       " [frozenset({2, 3}), frozenset({3, 5}), frozenset({2, 5}), frozenset({1, 3})],\n",
       " [frozenset({2, 3, 5})],\n",
       " []]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[frozenset({5}), frozenset({2}), frozenset({3}), frozenset({1})]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[frozenset({2, 3}), frozenset({3, 5}), frozenset({2, 5}), frozenset({1, 3})]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[frozenset({2, 3, 5})]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L[3] # Since output is empty, we stop here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes the list of all frequent itemsets and mines the association rules from it\n",
    "# It returns a list of association rules that meet the given minConf criteria of 0.7\n",
    "# supportData \n",
    "def generateRules(L, supportData, minConf=0.7):  # supportData is a dict coming from scanDataSet\n",
    "    bigRuleList = []\n",
    "    for i in range(1, len(L)): # only get the sets with two or more items\n",
    "        for freqSet in L[i]:\n",
    "            H1 = [frozenset([item]) for item in freqSet]\n",
    "            if (i > 1):\n",
    "                rulesFromConseq(freqSet, H1, supportData, bigRuleList, minConf)\n",
    "            else:\n",
    "                calcConf(freqSet, H1, supportData, bigRuleList, minConf)\n",
    "    return bigRuleList  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcConf(freqSet, H, supportData, brl, minConf=0.7):\n",
    "    prunedH = [] # create new list to return\n",
    "    for conseq in H:\n",
    "        temp_val = supportData[freqSet - conseq]\n",
    "        conf = supportData[freqSet] / supportData[freqSet - conseq]\n",
    "        if conf >= minConf: \n",
    "            print (freqSet-conseq,'-->',conseq,'conf:',conf)\n",
    "            brl.append((freqSet-conseq, conseq, conf))\n",
    "            prunedH.append(conseq)\n",
    "    return prunedH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rulesFromConseq(freqSet, H, supportData, brl, minConf=0.7):\n",
    "    m = len(H[0])\n",
    "    if (len(freqSet) > (m + 1)): # try further merging\n",
    "        Hmp1 = aprioriAlgorithm(H, m+1) # create Hm+1 new candidates\n",
    "        Hmp1 = calcConf(freqSet, Hmp1, supportData, brl, minConf)\n",
    "        if (len(Hmp1) > 1):    # need at least two sets to merge\n",
    "            rulesFromConseq(freqSet, Hmp1, supportData, brl, minConf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "L,suppData= aprioriAlgorithm(dataset,minSupport=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({5}) --> frozenset({2}) conf: 1.0\n",
      "frozenset({2}) --> frozenset({5}) conf: 1.0\n",
      "frozenset({1}) --> frozenset({3}) conf: 1.0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'frozenset' and 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-a7733baec573>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrules\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mgenerateRules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msuppData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminConf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-22-a8ac4562f787>\u001b[0m in \u001b[0;36mgenerateRules\u001b[0;34m(L, supportData, minConf)\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mH1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfrozenset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfreqSet\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                 \u001b[0mrulesFromConseq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfreqSet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupportData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbigRuleList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminConf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0mcalcConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfreqSet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupportData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbigRuleList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminConf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-82fbad90bf5e>\u001b[0m in \u001b[0;36mrulesFromConseq\u001b[0;34m(freqSet, H, supportData, brl, minConf)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfreqSet\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# try further merging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mHmp1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maprioriAlgorithm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# create Hm+1 new candidates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mHmp1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalcConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfreqSet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHmp1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupportData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbrl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminConf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHmp1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# need at least two sets to merge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mrulesFromConseq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfreqSet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHmp1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupportData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbrl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminConf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-cb8e6a0abf5e>\u001b[0m in \u001b[0;36mcalcConf\u001b[0;34m(freqSet, H, supportData, brl, minConf)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprunedH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# create new list to return\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mconseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mtemp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msupportData\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfreqSet\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mconseq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msupportData\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfreqSet\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0msupportData\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfreqSet\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mconseq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconf\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mminConf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'frozenset' and 'list'"
     ]
    }
   ],
   "source": [
    "rules= generateRules(L,suppData, minConf=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Apriori -\n",
      "Minimum Support: 0.25, Minimum Confidence: 0.8\n",
      "Transactions:\n",
      "\t[1, 2, 3, 4]\n",
      "\t[1, 2, 4]\n",
      "\t[1, 2]\n",
      "\t[2, 3, 4]\n",
      "\t[2, 3]\n",
      "\t[3, 4]\n",
      "\t[2, 4]\n",
      "Frequent Itemsets:\n",
      "\t[1, 2, 3, 4, [1, 2], [1, 4], [2, 3], [2, 4], [3, 4], [1, 2, 4], [2, 3, 4]]\n",
      "Rules:\n",
      "\t1 -> 2 (support: 0.43, confidence: 1.0)\n",
      "\t4 -> 2 (support: 0.57, confidence: 0.8)\n",
      "\t[1, 4] -> 2 (support: 0.29, confidence: 1.0)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from __future__ import division, print_function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "\n",
    "class Rule():\n",
    "    def __init__(self, antecedent, concequent, confidence, support):\n",
    "        self.antecedent = antecedent\n",
    "        self.concequent = concequent\n",
    "        self.confidence = confidence\n",
    "        self.support = support\n",
    "\n",
    "\n",
    "class Apriori():\n",
    "    \"\"\"A method for determining frequent itemsets in a transactional database and\n",
    "    also for generating rules for those itemsets.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    min_sup: float\n",
    "        The minimum fraction of transactions an itemets needs to\n",
    "        occur in to be deemed frequent\n",
    "    min_conf: float:\n",
    "        The minimum fraction of times the antecedent needs to imply\n",
    "        the concequent to justify rule\n",
    "    \"\"\"\n",
    "    def __init__(self, min_sup=0.3, min_conf=0.81):\n",
    "\n",
    "        self.min_sup = min_sup\n",
    "        self.min_conf = min_conf\n",
    "        self.freq_itemsets = None       # List of freqeuent itemsets\n",
    "        self.transactions = None        # List of transactions\n",
    "\n",
    "    def _calculate_support(self, itemset):\n",
    "        count = 0\n",
    "        for transaction in self.transactions:\n",
    "            if self._transaction_contains_items(transaction, itemset):\n",
    "                count += 1\n",
    "        support = count / len(self.transactions)\n",
    "        return support\n",
    "\n",
    "    # Prunes the candidates that are not frequent\n",
    "    # => returns list with only frequent itemsets\n",
    "    def _get_frequent_itemsets(self, candidates):\n",
    "        frequent = []\n",
    "        # Find frequent items\n",
    "        for itemset in candidates:\n",
    "            support = self._calculate_support(itemset)\n",
    "            if support >= self.min_sup:\n",
    "                frequent.append(itemset)\n",
    "        return frequent\n",
    "\n",
    "    # True or false depending on the candidate has any\n",
    "    # subset with size k - 1 that is not in the frequent\n",
    "    # itemset\n",
    "    def _has_infrequent_itemsets(self, candidate):\n",
    "        k = len(candidate)\n",
    "        # Find all combinations of size k-1 in candidate\n",
    "        # E.g [1,2,3] => [[1,2],[1,3],[2,3]]\n",
    "        subsets = list(itertools.combinations(candidate, k - 1))\n",
    "        for t in subsets:\n",
    "            # t - is tuple. If size == 1 get the element\n",
    "            subset = list(t) if len(t) > 1 else t[0]\n",
    "            if not subset in self.freq_itemsets[-1]:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    # Joins the elements in the frequent itemset and prunes\n",
    "    # resulting sets if they contain subsets that have been determined\n",
    "    # to be infrequent.\n",
    "    def _generate_candidates(self, freq_itemset):\n",
    "        candidates = []\n",
    "        for itemset1 in freq_itemset:\n",
    "            for itemset2 in freq_itemset:\n",
    "                # Valid if every element but the last are the same\n",
    "                # and the last element in itemset1 is smaller than the last\n",
    "                # in itemset2\n",
    "                valid = False\n",
    "                single_item = isinstance(itemset1, int)\n",
    "                if single_item and itemset1 < itemset2:\n",
    "                    valid = True\n",
    "                elif not single_item and np.array_equal(itemset1[:-1], itemset2[:-1]) and itemset1[-1] < itemset2[-1]:\n",
    "                    valid = True\n",
    "\n",
    "                if valid:\n",
    "                    # JOIN: Add the last element in itemset2 to itemset1 to\n",
    "                    # create a new candidate\n",
    "                    if single_item:\n",
    "                        candidate = [itemset1, itemset2]\n",
    "                    else:\n",
    "                        candidate = itemset1 + [itemset2[-1]]\n",
    "                    # PRUNE: Check if any subset of candidate have been determined\n",
    "                    # to be infrequent\n",
    "                    infrequent = self._has_infrequent_itemsets(candidate)\n",
    "                    if not infrequent:\n",
    "                        candidates.append(candidate)\n",
    "        return candidates\n",
    "\n",
    "    # True or false depending on each item in the itemset is\n",
    "    # in the transaction\n",
    "    def _transaction_contains_items(self, transaction, items):\n",
    "        # If items is in fact only one item\n",
    "        if isinstance(items, int):\n",
    "            return items in transaction\n",
    "        # Iterate through list of items and make sure that\n",
    "        # all items are in the transaction\n",
    "        for item in items:\n",
    "            if not item in transaction:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    # Returns the set of frequent itemsets in the list of transactions\n",
    "    def find_frequent_itemsets(self, transactions):\n",
    "        self.transactions = transactions\n",
    "        # Get all unique items in the transactions\n",
    "        unique_items = set(item for transaction in self.transactions for item in transaction)\n",
    "        # Get the frequent items\n",
    "        self.freq_itemsets = [self._get_frequent_itemsets(unique_items)]\n",
    "        while(True):\n",
    "            # Generate new candidates from last added frequent itemsets\n",
    "            candidates = self._generate_candidates(self.freq_itemsets[-1])\n",
    "            # Get the frequent itemsets among those candidates\n",
    "            frequent_itemsets = self._get_frequent_itemsets(candidates)\n",
    "\n",
    "            # If there are no frequent itemsets we're done\n",
    "            if not frequent_itemsets:\n",
    "                break\n",
    "\n",
    "            # Add them to the total list of frequent itemsets and start over\n",
    "            self.freq_itemsets.append(frequent_itemsets)\n",
    "\n",
    "        # Flatten the array and return every frequent itemset\n",
    "        frequent_itemsets = [\n",
    "            itemset for sublist in self.freq_itemsets for itemset in sublist]\n",
    "        return frequent_itemsets\n",
    "\n",
    "    # Recursive function which returns the rules where confidence >= min_confidence\n",
    "    # Starts with large itemset and recursively explores rules for subsets\n",
    "    def _rules_from_itemset(self, initial_itemset, itemset):\n",
    "        rules = []\n",
    "        k = len(itemset)\n",
    "        # Get all combinations of sub-itemsets of size k - 1 from itemset\n",
    "        # E.g [1,2,3] => [[1,2],[1,3],[2,3]]\n",
    "        subsets = list(itertools.combinations(itemset, k - 1))\n",
    "        support = self._calculate_support(initial_itemset)\n",
    "        for antecedent in subsets:\n",
    "            # itertools.combinations returns tuples => convert to list\n",
    "            antecedent = list(antecedent)\n",
    "            antecedent_support = self._calculate_support(antecedent)\n",
    "            # Calculate the confidence as sup(A and B) / sup(B), if antecedent\n",
    "            # is B in an itemset of A and B\n",
    "            confidence = float(\"{0:.2f}\".format(support / antecedent_support))\n",
    "            if confidence >= self.min_conf:\n",
    "                # The concequent is the initial_itemset except for antecedent\n",
    "                concequent = [itemset for itemset in initial_itemset if not itemset in antecedent]\n",
    "                # If single item => get item\n",
    "                if len(antecedent) == 1:\n",
    "                    antecedent = antecedent[0]\n",
    "                if len(concequent) == 1:\n",
    "                    concequent = concequent[0]\n",
    "                # Create new rule\n",
    "                rule = Rule(\n",
    "                        antecedent=antecedent,\n",
    "                        concequent=concequent,\n",
    "                        confidence=confidence,\n",
    "                        support=support)\n",
    "                rules.append(rule)\n",
    "\n",
    "                # If there are subsets that could result in rules\n",
    "                # recursively add rules from subsets\n",
    "                if k - 1 > 1:\n",
    "                    rules += self._rules_from_itemset(initial_itemset, antecedent)\n",
    "        return rules\n",
    "\n",
    "    def generate_rules(self, transactions):\n",
    "        self.transactions = transactions\n",
    "        frequent_itemsets = self.find_frequent_itemsets(transactions)\n",
    "        # Only consider itemsets of size >= 2 items\n",
    "        frequent_itemsets = [itemset for itemset in frequent_itemsets if not isinstance(\n",
    "                itemset, int)]\n",
    "        rules = []\n",
    "        for itemset in frequent_itemsets:\n",
    "            rules += self._rules_from_itemset(itemset, itemset)\n",
    "        # Remove empty values\n",
    "        return rules\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Demo transaction set\n",
    "    # Example 2: https://en.wikipedia.org/wiki/Apriori_algorithm\n",
    "    transactions = np.array([[1, 2, 3, 4], [1, 2, 4], [1, 2], [2, 3, 4], [2, 3], [3, 4], [2, 4]])\n",
    "    print (\"- Apriori -\")\n",
    "    min_sup = 0.25\n",
    "    min_conf = 0.8\n",
    "    print (\"Minimum Support: %.2f, Minimum Confidence: %s\" % (min_sup, min_conf))\n",
    "    print (\"Transactions:\")\n",
    "    for transaction in transactions:\n",
    "        print (\"\\t%s\" % transaction)\n",
    "\n",
    "    apriori = Apriori(min_sup=min_sup, min_conf=min_conf)\n",
    "\n",
    "    # Get and print the frequent itemsets\n",
    "    frequent_itemsets = apriori.find_frequent_itemsets(transactions)\n",
    "    print (\"Frequent Itemsets:\\n\\t%s\" % frequent_itemsets)\n",
    "\n",
    "    # Get and print the rules\n",
    "    rules = apriori.generate_rules(transactions)\n",
    "    print (\"Rules:\")\n",
    "    for rule in rules:\n",
    "        print (\"\\t%s -> %s (support: %.2f, confidence: %s)\" % (rule.antecedent, rule.concequent, rule.support, rule.confidence,))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
